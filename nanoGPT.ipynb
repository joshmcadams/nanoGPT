{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnMP4VEhpytU2UBUg3YLJF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshmcadams/nanoGPT/blob/main/nanoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nanoGPT"
      ],
      "metadata": {
        "id": "AfzhVUNsm6yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nanoGPT is a wonderful exploration into building a GPT model from scratch. We'll start with an empty notebook and quickly build and train a model to generate Shakespeare-like text.\n",
        "\n",
        "This notebook is based off of a code and videos releasted by Andrej Karpathy. Please be sure to check out his work!\n",
        "\n",
        "  * [Reference Video Lecture](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
        "  * [Reference Git Repository](https://github.com/karpathy/ng-video-lecture)"
      ],
      "metadata": {
        "id": "0R5juEnSnABo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration and Preparation"
      ],
      "metadata": {
        "id": "sHvhw5YXotHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquiring the data"
      ],
      "metadata": {
        "id": "k6YgbsEVo3BE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll first pull the data down into this lab. To get the data the `wget` command will be used in the shell."
      ],
      "metadata": {
        "id": "agEnmOPeo46n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffHXHUq9pGTf",
        "outputId": "ecb72a78-5879-46b1-806f-bba928a21dcb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-12 05:05:57--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-12 05:05:57 (31.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll load the data into Python and take a look at a sample."
      ],
      "metadata": {
        "id": "s8L7q9zLpNfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we simply open the file and store the entire contents of the file in the variable `raw_training_data`. We get the length of the data and print that out so that we can see how much data we are dealing with."
      ],
      "metadata": {
        "id": "GhKBNFmnqBLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_training_data = f.read()\n",
        "\n",
        "training_data_size = len(raw_training_data)\n",
        "print(training_data_size)"
      ],
      "metadata": {
        "id": "89fV4YklpUvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51523879-93c0-4f6a-85c1-0d5cf506edfc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we sample the data just to get an idea of what we are dealing with. The sample comes from a random locaiton in the data. Run the code block below a few times to see some different data samples."
      ],
      "metadata": {
        "id": "uDj696OuqXdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "SAMPLE_SIZE = 500\n",
        "start = random.randrange(0, training_data_size - SAMPLE_SIZE)\n",
        "print(raw_training_data[start:start+SAMPLE_SIZE])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PaomKtIpZWH",
        "outputId": "3259450b-0f31-4d3d-bffa-0b7f85da953c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ed\n",
            "putting-on; methinks strangely, for he hath not used it before.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Pray you, let's hear.\n",
            "\n",
            "Provost:\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "What is that Barnardine who is to be executed in the\n",
            "afternoon?\n",
            "\n",
            "Provost:\n",
            "A Bohemian born, but here nursed un and bred; one\n",
            "that is a prisoner nine years old.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "How came it that the absent duke had not either\n",
            "delivered him to his liberty or executed him? I\n",
            "have heard it was ever his manner to do so.\n",
            "\n",
            "Provost:\n",
            "His friends still wrought reprieves for h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis"
      ],
      "metadata": {
        "id": "AqrIITOTrJah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've now acquired the data and even poked around a bit to see what the data looks like. However, we need to dig deeper and do some more detailed analysis on the data."
      ],
      "metadata": {
        "id": "cJ9f9lIerRpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important fact to know about data that will be fed into a generative model is how many different characters are we working with. This is actually really easy in Python and can be accomplished using the `set` function."
      ],
      "metadata": {
        "id": "CQm6MowVrgQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sorted(list(set(raw_training_data)))\n",
        "token_count = len(tokens)\n",
        "\n",
        "print(f'There are {token_count} unique characters in the {training_data_size}' +\n",
        "      f'of training data. The characters are: {\"\".join(tokens)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEDbtcArrfx_",
        "outputId": "44ac82af-08f9-41ad-a726-4fb0a9a5b304"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 65 unique characters in the 1115394of training data. The characters are: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "jwLBJJ2TtP7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now know how many different unique tokens exist in our training data. In our case, these tokens will be individual characters, but this a choice that we have made. We could have alternatively chosen pairs, triplets, or some other combination of characters to be a token.\n",
        "\n",
        "Regardless of the data that comprises our tokens, the token itself isn't what we feed to the model. Instead, we need to convert the token into a numeric representation since models are performing numeric calculations internally.\n",
        "\n",
        "For our character-based tokens we could just use the ASCII value of the character using the `ord` function. However, you might eventually want to expermiment with sequences of characters as tokens, and in this case you'll need a more complex token-to-number mapping, so lets just create a token-to-number mapping scheme now.\n",
        "\n",
        "Since we ordered our unique tokens into the `tokens` list, we can just use the index of the token in the list as the encoding, which is what we do.\n",
        "\n",
        "Note that this does come with some trade-offs though. The position of any given token in the list of tokens is relative to how many and what were the unique tokens in the training data. If we use different trianing data, the tokens might map to different indexes so we need to be careful to preserve our tokens across training data set if we are using mulitple different pieces of training data.\n",
        "\n",
        "Anyway, let's write a token-to-number `encode` function and a number-to-token `decode` function."
      ],
      "metadata": {
        "id": "eENRH2pytS-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, create a mapping of tokens to numbers."
      ],
      "metadata": {
        "id": "x1ybdTibvhg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_to_number = {t: i for i, t in enumerate(tokens)}\n",
        "token_to_number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO-Y4yjkvNM3",
        "outputId": "8b8bd34c-92fb-414f-b80b-82e3d25e8fa8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '$': 3,\n",
              " '&': 4,\n",
              " \"'\": 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '3': 9,\n",
              " ':': 10,\n",
              " ';': 11,\n",
              " '?': 12,\n",
              " 'A': 13,\n",
              " 'B': 14,\n",
              " 'C': 15,\n",
              " 'D': 16,\n",
              " 'E': 17,\n",
              " 'F': 18,\n",
              " 'G': 19,\n",
              " 'H': 20,\n",
              " 'I': 21,\n",
              " 'J': 22,\n",
              " 'K': 23,\n",
              " 'L': 24,\n",
              " 'M': 25,\n",
              " 'N': 26,\n",
              " 'O': 27,\n",
              " 'P': 28,\n",
              " 'Q': 29,\n",
              " 'R': 30,\n",
              " 'S': 31,\n",
              " 'T': 32,\n",
              " 'U': 33,\n",
              " 'V': 34,\n",
              " 'W': 35,\n",
              " 'X': 36,\n",
              " 'Y': 37,\n",
              " 'Z': 38,\n",
              " 'a': 39,\n",
              " 'b': 40,\n",
              " 'c': 41,\n",
              " 'd': 42,\n",
              " 'e': 43,\n",
              " 'f': 44,\n",
              " 'g': 45,\n",
              " 'h': 46,\n",
              " 'i': 47,\n",
              " 'j': 48,\n",
              " 'k': 49,\n",
              " 'l': 50,\n",
              " 'm': 51,\n",
              " 'n': 52,\n",
              " 'o': 53,\n",
              " 'p': 54,\n",
              " 'q': 55,\n",
              " 'r': 56,\n",
              " 's': 57,\n",
              " 't': 58,\n",
              " 'u': 59,\n",
              " 'v': 60,\n",
              " 'w': 61,\n",
              " 'x': 62,\n",
              " 'y': 63,\n",
              " 'z': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then a mapping from numbers to tokens."
      ],
      "metadata": {
        "id": "C3PcZhzXvyzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_to_token = {i: t for i, t in enumerate(tokens)}\n",
        "number_to_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjOQwnVGvtBE",
        "outputId": "3667b8d4-3d4c-4cf7-81ab-2c38e8b48e34"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: ' ',\n",
              " 2: '!',\n",
              " 3: '$',\n",
              " 4: '&',\n",
              " 5: \"'\",\n",
              " 6: ',',\n",
              " 7: '-',\n",
              " 8: '.',\n",
              " 9: '3',\n",
              " 10: ':',\n",
              " 11: ';',\n",
              " 12: '?',\n",
              " 13: 'A',\n",
              " 14: 'B',\n",
              " 15: 'C',\n",
              " 16: 'D',\n",
              " 17: 'E',\n",
              " 18: 'F',\n",
              " 19: 'G',\n",
              " 20: 'H',\n",
              " 21: 'I',\n",
              " 22: 'J',\n",
              " 23: 'K',\n",
              " 24: 'L',\n",
              " 25: 'M',\n",
              " 26: 'N',\n",
              " 27: 'O',\n",
              " 28: 'P',\n",
              " 29: 'Q',\n",
              " 30: 'R',\n",
              " 31: 'S',\n",
              " 32: 'T',\n",
              " 33: 'U',\n",
              " 34: 'V',\n",
              " 35: 'W',\n",
              " 36: 'X',\n",
              " 37: 'Y',\n",
              " 38: 'Z',\n",
              " 39: 'a',\n",
              " 40: 'b',\n",
              " 41: 'c',\n",
              " 42: 'd',\n",
              " 43: 'e',\n",
              " 44: 'f',\n",
              " 45: 'g',\n",
              " 46: 'h',\n",
              " 47: 'i',\n",
              " 48: 'j',\n",
              " 49: 'k',\n",
              " 50: 'l',\n",
              " 51: 'm',\n",
              " 52: 'n',\n",
              " 53: 'o',\n",
              " 54: 'p',\n",
              " 55: 'q',\n",
              " 56: 'r',\n",
              " 57: 's',\n",
              " 58: 't',\n",
              " 59: 'u',\n",
              " 60: 'v',\n",
              " 61: 'w',\n",
              " 62: 'x',\n",
              " 63: 'y',\n",
              " 64: 'z'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can write our encoder."
      ],
      "metadata": {
        "id": "hVd0DxSKv2-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(tokens: str) -> list[int]:\n",
        "  return [token_to_number[t] for t in tokens]\n",
        "\n",
        "encode('apple')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "775B6Tbov2ux",
        "outputId": "baee30be-3737-4e2a-e74f-22b879ad4fce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[39, 54, 54, 50, 43]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the decoder."
      ],
      "metadata": {
        "id": "5RrUVwhlwEB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(numbers: list[int]) -> str:\n",
        "  return ''.join(number_to_token[n] for n in numbers)\n",
        "\n",
        "decode([16, 17, 18])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dfDycwx5wGWF",
        "outputId": "a1c7cb4c-aae1-4ce2-e19c-fe99e3dc1a6c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DEF'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a tokenizer and a large amount of text. Let's tokenize all the text!\n",
        "\n",
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=778)"
      ],
      "metadata": {
        "id": "cTdGKKUD5PZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(raw_training_data), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUZ4022h5Z38",
        "outputId": "6bcfa817-ffcb-4c14-8283-11e471f6f7f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to split training data and testing data."
      ],
      "metadata": {
        "id": "C6w2aQ4-6IYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_size = int(0.9*len(data))\n",
        "training_data = data[:training_data_size]\n",
        "validation_data = data[training_data_size:]\n",
        "print(len(training_data), len(validation_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYVyMdov6MOF",
        "outputId": "7da485ef-d479-4e2d-df0a-65e999cc49a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Dimension"
      ],
      "metadata": {
        "id": "N-ktY-_58eLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we work on training in blocks.\n",
        "In this case we train in blocks of 8 characters, but we always need the \"next\" character, so we work with block sizes of bock+1.\n",
        "\n",
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=913)"
      ],
      "metadata": {
        "id": "8ZdjI7bR7DY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "training_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrxJhNAH7I2u",
        "outputId": "a340f88d-5e06-48b4-d747-dcecf5fbbbba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = training_data[:block_size]\n",
        "y = training_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"in: {context}, out: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB3aEKF77jz1",
        "outputId": "34058735-d3c3-4b1e-f5bb-7925617133e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in: tensor([18]), out: 47\n",
            "in: tensor([18, 47]), out: 56\n",
            "in: tensor([18, 47, 56]), out: 57\n",
            "in: tensor([18, 47, 56, 57]), out: 58\n",
            "in: tensor([18, 47, 56, 57, 58]), out: 1\n",
            "in: tensor([18, 47, 56, 57, 58,  1]), out: 15\n",
            "in: tensor([18, 47, 56, 57, 58,  1, 15]), out: 47\n",
            "in: tensor([18, 47, 56, 57, 58,  1, 15, 47]), out: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Dimension"
      ],
      "metadata": {
        "id": "XScEDiyd8he6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=1129)"
      ],
      "metadata": {
        "id": "SwOcHO4i8sNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = training_data if split == 'train' else validation_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "print('----------------------------------')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f'input {context.tolist()}, target {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDrFtzol8l9R",
        "outputId": "e182f25e-79af-494f-f5db-b2e48103826c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----------------------------------\n",
            "input [24], target 43\n",
            "input [24, 43], target 58\n",
            "input [24, 43, 58], target 5\n",
            "input [24, 43, 58, 5], target 57\n",
            "input [24, 43, 58, 5, 57], target 1\n",
            "input [24, 43, 58, 5, 57, 1], target 46\n",
            "input [24, 43, 58, 5, 57, 1, 46], target 43\n",
            "input [24, 43, 58, 5, 57, 1, 46, 43], target 39\n",
            "input [44], target 53\n",
            "input [44, 53], target 56\n",
            "input [44, 53, 56], target 1\n",
            "input [44, 53, 56, 1], target 58\n",
            "input [44, 53, 56, 1, 58], target 46\n",
            "input [44, 53, 56, 1, 58, 46], target 39\n",
            "input [44, 53, 56, 1, 58, 46, 39], target 58\n",
            "input [44, 53, 56, 1, 58, 46, 39, 58], target 1\n",
            "input [52], target 58\n",
            "input [52, 58], target 1\n",
            "input [52, 58, 1], target 58\n",
            "input [52, 58, 1, 58], target 46\n",
            "input [52, 58, 1, 58, 46], target 39\n",
            "input [52, 58, 1, 58, 46, 39], target 58\n",
            "input [52, 58, 1, 58, 46, 39, 58], target 1\n",
            "input [52, 58, 1, 58, 46, 39, 58, 1], target 46\n",
            "input [25], target 17\n",
            "input [25, 17], target 27\n",
            "input [25, 17, 27], target 10\n",
            "input [25, 17, 27, 10], target 0\n",
            "input [25, 17, 27, 10, 0], target 21\n",
            "input [25, 17, 27, 10, 0, 21], target 1\n",
            "input [25, 17, 27, 10, 0, 21, 1], target 54\n",
            "input [25, 17, 27, 10, 0, 21, 1, 54], target 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram Model"
      ],
      "metadata": {
        "id": "w2bRuY9s-3KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "YqUO5IDzGWae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=1348)\n",
        "\n",
        "[Makemore Series covering the BiGram language model](https://www.youtube.com/watch?v=PaCmpygFfXo)"
      ],
      "metadata": {
        "id": "RN-o8VBS-7PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    B, T, C = logits.shape\n",
        "    logits = logits.view(B*T, C)\n",
        "    targets = targets.view(B*T)\n",
        "    loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "m = BigramLanguageModel(token_count)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssEOig2v-4rF",
        "outputId": "cf45409a-b5b0-432f-bd48-32b63b177a46"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(-np.log(1/65.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMXTrz6PCMoC",
        "outputId": "2dd48919-dec2-4352-ebf0-99f537bd25d0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.174387269895637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=1753)"
      ],
      "metadata": {
        "id": "FixxdvCCC4fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(token_count)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "z = torch.zeros((1,1), dtype=torch.long)\n",
        "print(z)\n",
        "generated = m.generate(z, max_new_tokens=100)\n",
        "print(generated)\n",
        "generated_list = generated[0].tolist()\n",
        "print(generated_list)\n",
        "print(decode(generated_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H59A9IvECueF",
        "outputId": "bfa0044c-ca14-4d38-e555-c1cfca914eca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "tensor([[0]])\n",
            "tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16, 52, 62, 13,  1, 25, 57,  3,  9]])\n",
            "[0, 31, 56, 12, 55, 28, 7, 29, 35, 49, 58, 36, 53, 24, 4, 48, 24, 16, 22, 45, 27, 24, 34, 64, 5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34, 4, 60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63, 39, 53, 8, 55, 44, 64, 57, 3, 37, 57, 3, 64, 18, 7, 61, 6, 11, 43, 17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57, 2, 47, 35, 35, 8, 27, 40, 64, 16, 52, 62, 13, 1, 25, 57, 3, 9]\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ABoqGahpGYi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=2092)"
      ],
      "metadata": {
        "id": "EnltATAZGZwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "qcn8rnsxGcUO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "for steps in range(10000):\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j86g3U1YMfXT",
        "outputId": "dc74c383-59be-4d0e-e4d5-262e6eeeeaae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.559082508087158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.zeros((1,1), dtype=torch.long)\n",
        "print(z)\n",
        "generated = m.generate(z, max_new_tokens=100)\n",
        "print(generated)\n",
        "generated_list = generated[0].tolist()\n",
        "print(generated_list)\n",
        "print(decode(generated_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xEql-p7NGLf",
        "outputId": "b39b36b8-249d-4ef9-ee65-5e052856ad88"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0]])\n",
            "tensor([[ 0, 13, 52, 41, 53,  1, 57,  1, 39, 54, 56, 45, 56,  1, 39, 50, 57,  1,\n",
            "         53, 56, 43, 56, 60, 43,  6,  1, 63, 53,  1, 47, 42, 47, 50, 39, 41, 39,\n",
            "         47, 39,  1, 44,  1, 58, 50, 50, 47, 52, 45, 47, 58, 56, 43, 63,  1, 50,\n",
            "          1, 53, 59, 43, 60, 43, 56, 58,  1, 15, 33, 30, 53, 56, 42,  1, 40, 43,\n",
            "         39,  1, 39, 56, 53, 59, 50, 50, 39, 47, 53, 61, 53, 52, 45, 43,  6,  1,\n",
            "         57,  8,  0, 25, 63,  1, 63,  1, 48, 53, 51]])\n",
            "[0, 13, 52, 41, 53, 1, 57, 1, 39, 54, 56, 45, 56, 1, 39, 50, 57, 1, 53, 56, 43, 56, 60, 43, 6, 1, 63, 53, 1, 47, 42, 47, 50, 39, 41, 39, 47, 39, 1, 44, 1, 58, 50, 50, 47, 52, 45, 47, 58, 56, 43, 63, 1, 50, 1, 53, 59, 43, 60, 43, 56, 58, 1, 15, 33, 30, 53, 56, 42, 1, 40, 43, 39, 1, 39, 56, 53, 59, 50, 50, 39, 47, 53, 61, 53, 52, 45, 43, 6, 1, 57, 8, 0, 25, 63, 1, 63, 1, 48, 53, 51]\n",
            "\n",
            "Anco s aprgr als orerve, yo idilacaia f tllingitrey l ouevert CURord bea aroullaiowonge, s.\n",
            "My y jom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When the code is converted to a script\n"
      ],
      "metadata": {
        "id": "yxuEmNgQNnpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Checkpoint](https://youtu.be/kCc8FmEb1nY?t=2257)"
      ],
      "metadata": {
        "id": "uOWmir7ENRJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKNXTIm2NrRT",
        "outputId": "2f1c9c8e-739a-4d06-86e7-64f71de307bb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 losses {'train': tensor(4.7305), 'val': tensor(4.7241)}\n",
            "iter 300 losses {'train': tensor(2.8101), 'val': tensor(2.8243)}\n",
            "iter 600 losses {'train': tensor(2.5400), 'val': tensor(2.5652)}\n",
            "iter 900 losses {'train': tensor(2.4896), 'val': tensor(2.5060)}\n",
            "iter 1200 losses {'train': tensor(2.4830), 'val': tensor(2.5012)}\n",
            "iter 1500 losses {'train': tensor(2.4631), 'val': tensor(2.4905)}\n",
            "iter 1800 losses {'train': tensor(2.4655), 'val': tensor(2.4926)}\n",
            "iter 2100 losses {'train': tensor(2.4670), 'val': tensor(2.4840)}\n",
            "iter 2400 losses {'train': tensor(2.4612), 'val': tensor(2.4879)}\n",
            "iter 2700 losses {'train': tensor(2.4717), 'val': tensor(2.4915)}\n",
            "\n",
            "od nos CAy go ghanoray t, co haringoudrou clethe k, leve fr werar,\n",
            "Is fa!\n",
            "\n",
            "\n",
            "Thilemel cia h hmboomyorarifrcitheviPO, tle dst f qur'dig t cof boddo y t o ar pileas h mo wierl t,\n",
            "S:\n",
            "STENENEat I athe thounomy tinrent distesisanimald 3I'leliento ald, avavis nofrisist me Busarend un'soto iat s k,\n",
            "SBRI he the f wendleindd t acoe ts ansur thy ppr h.\n",
            "\n",
            "\n",
            "Y:\n",
            "KIIsqu pr odEd ch,\n",
            "APrnes ouse bll owhored miner t ooon'stoume bupromo! fifoveghind hiarnge s.\n",
            "MI aswimy or m, wardd tw'To tee abifewoetsphin sed The a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Math Trick Self-Attention"
      ],
      "metadata": {
        "id": "smnTceY1UmTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgoEoP4TUouq",
        "outputId": "8f64b672-c06f-4445-b797-f53630aac202"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1]\n",
        "    xbow[b,t] = torch.mean(xprev, 0)\n",
        "\n",
        "x[0], xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_JX5N6aVbCL",
        "outputId": "24743b06-22e4-4acb-8334-8b5f0f4d5b4b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700],\n",
              "         [-0.3596, -0.9152],\n",
              "         [ 0.6258,  0.0255],\n",
              "         [ 0.9545,  0.0643],\n",
              "         [ 0.3612,  1.1679],\n",
              "         [-1.3499, -0.5102],\n",
              "         [ 0.2360, -0.2398],\n",
              "         [-0.9211,  1.5433]]),\n",
              " tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.ones(3,3)\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('----------------')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('----------------')\n",
        "print('c=')\n",
        "print(c)\n",
        "print('----------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwjMV2GdV6el",
        "outputId": "7d27fd4f-bc4f-4789-cc83-cc79453d9d84"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "----------------\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "----------------\n",
            "c=\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tril(torch.ones(3,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4qePecXWSlE",
        "outputId": "c4bf3ae4-2618-4eca-ae3a-ab8f5cb15d73"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('----------------')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('----------------')\n",
        "print('c=')\n",
        "print(c)\n",
        "print('----------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n-qd_7qWinG",
        "outputId": "e6d64f9f-658f-42c0-bc55-8af9c5068b85"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "----------------\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "----------------\n",
            "c=\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('----------------')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('----------------')\n",
        "print('c=')\n",
        "print(c)\n",
        "print('----------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdsWB8LTW97V",
        "outputId": "f777a413-cab2-4432-bbad-8a9c49097a24"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "----------------\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "----------------\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm0fIChRXQ2F",
        "outputId": "e43f57a7-3771-4230-e9b6-04c24d38924a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C)\n",
        "torch.allclose(xbow[0], xbow2[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sizFtzMeXdoe",
        "outputId": "caa67c79-23f0-4302-8447-f36ac342be59"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0], xbow2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpNq46uoX4Zp",
        "outputId": "d3c52c82-77a3-4f41-d01b-cb1928728be3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]),\n",
              " tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow[0], xbow3[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATxsDtI8YORR",
        "outputId": "82ae0f18-16a5-43dc-c841-8c0e97433eea"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Self-Attention"
      ],
      "metadata": {
        "id": "XINMkAIyZm7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Checkpoint](https://youtu.be/kCc8FmEb1nY?t=3510)"
      ],
      "metadata": {
        "id": "TrDkjivdZtPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    logits = self.lm_head(tok_emb)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQhO50z5Zo15",
        "outputId": "650c3951-eba3-49ab-d144-40e8f90e2517"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0; train loss 4.3886; val loss 4.3734\n",
            "iter 300; train loss 2.5268; val loss 2.5406\n",
            "iter 600; train loss 2.5004; val loss 2.5331\n",
            "iter 900; train loss 2.4910; val loss 2.5101\n",
            "iter 1200; train loss 2.4985; val loss 2.5155\n",
            "iter 1500; train loss 2.4819; val loss 2.5046\n",
            "iter 1800; train loss 2.4864; val loss 2.5177\n",
            "iter 2100; train loss 2.4870; val loss 2.5024\n",
            "iter 2400; train loss 2.4892; val loss 2.5159\n",
            "iter 2700; train loss 2.5014; val loss 2.5142\n",
            "\n",
            "balyorerasicallcod nos CAneno ghalousord, co hakingoudre h wethe kes une fr werar,\n",
            "Is falo he.\n",
            "IUCis cou. ppr:\n",
            "\n",
            "\n",
            "IOKarifrcithevico, tld dst f Couced thin f boddo y t o ar pileas h mavimerl t,\n",
            "S:\n",
            "STo'd cackepathe thounomertinrent distesisanimald 3 m ensento ald, avaviconofrisist menghallpr, un'soto iat s kn, use hepute f fendleindd t acoe ts ansur thy ppr h.\n",
            "\n",
            "\n",
            "Y:\n",
            "KIIsqu pr oded char whave ousenlll owhofrd miner t ooon'stoume wh domo! fifoveghind hiarnge s.\n",
            "MENaswimy ojomes ard'd; 'To tee abw beme\n"
          ]
        }
      ]
    }
  ]
}