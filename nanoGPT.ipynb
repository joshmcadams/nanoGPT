{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhs5alK8xNMjPDpoAwrQ7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshmcadams/nanoGPT/blob/main/nanoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nanoGPT"
      ],
      "metadata": {
        "id": "AfzhVUNsm6yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nanoGPT is a wonderful exploration into building a GPT model from scratch. We'll start with an empty notebook and quickly build and train a model to generate Shakespeare-like text.\n",
        "\n",
        "This notebook is based off of a code and videos releasted by Andrej Karpathy. Please be sure to check out his work!\n",
        "\n",
        "  * [Reference Video Lecture](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
        "  * [Reference Git Repository](https://github.com/karpathy/ng-video-lecture)"
      ],
      "metadata": {
        "id": "0R5juEnSnABo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration and Preparation"
      ],
      "metadata": {
        "id": "sHvhw5YXotHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquiring the data"
      ],
      "metadata": {
        "id": "k6YgbsEVo3BE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll first pull the data down into this lab. To get the data the `wget` command will be used in the shell."
      ],
      "metadata": {
        "id": "agEnmOPeo46n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffHXHUq9pGTf",
        "outputId": "967edef2-b585-4137-8c54-c55da4a089d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-12 07:38:51--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2024-05-12 07:38:51 (184 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll load the data into Python and take a look at a sample."
      ],
      "metadata": {
        "id": "s8L7q9zLpNfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we simply open the file and store the entire contents of the file in the variable `raw_training_data`. We get the length of the data and print that out so that we can see how much data we are dealing with."
      ],
      "metadata": {
        "id": "GhKBNFmnqBLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_training_data = f.read()\n",
        "\n",
        "training_data_size = len(raw_training_data)\n",
        "print(training_data_size)"
      ],
      "metadata": {
        "id": "89fV4YklpUvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c556282-acfe-486e-a71b-a568f4055c26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we sample the data just to get an idea of what we are dealing with. The sample comes from a random locaiton in the data. Run the code block below a few times to see some different data samples."
      ],
      "metadata": {
        "id": "uDj696OuqXdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "SAMPLE_SIZE = 500\n",
        "start = random.randrange(0, training_data_size - SAMPLE_SIZE)\n",
        "print(raw_training_data[start:start+SAMPLE_SIZE])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PaomKtIpZWH",
        "outputId": "3259450b-0f31-4d3d-bffa-0b7f85da953c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ed\n",
            "putting-on; methinks strangely, for he hath not used it before.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Pray you, let's hear.\n",
            "\n",
            "Provost:\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "What is that Barnardine who is to be executed in the\n",
            "afternoon?\n",
            "\n",
            "Provost:\n",
            "A Bohemian born, but here nursed un and bred; one\n",
            "that is a prisoner nine years old.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "How came it that the absent duke had not either\n",
            "delivered him to his liberty or executed him? I\n",
            "have heard it was ever his manner to do so.\n",
            "\n",
            "Provost:\n",
            "His friends still wrought reprieves for h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis"
      ],
      "metadata": {
        "id": "AqrIITOTrJah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've now acquired the data and even poked around a bit to see what the data looks like. However, we need to dig deeper and do some more detailed analysis on the data."
      ],
      "metadata": {
        "id": "cJ9f9lIerRpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important fact to know about data that will be fed into a generative model is how many different characters are we working with. This is actually really easy in Python and can be accomplished using the `set` function."
      ],
      "metadata": {
        "id": "CQm6MowVrgQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sorted(list(set(raw_training_data)))\n",
        "token_count = len(tokens)\n",
        "\n",
        "print(f'There are {token_count} unique characters in the {training_data_size}' +\n",
        "      f'of training data. The characters are: {\"\".join(tokens)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEDbtcArrfx_",
        "outputId": "44ac82af-08f9-41ad-a726-4fb0a9a5b304"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 65 unique characters in the 1115394of training data. The characters are: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "jwLBJJ2TtP7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now know how many different unique tokens exist in our training data. In our case, these tokens will be individual characters, but this a choice that we have made. We could have alternatively chosen pairs, triplets, or some other combination of characters to be a token.\n",
        "\n",
        "Regardless of the data that comprises our tokens, the token itself isn't what we feed to the model. Instead, we need to convert the token into a numeric representation since models are performing numeric calculations internally.\n",
        "\n",
        "For our character-based tokens we could just use the ASCII value of the character using the `ord` function. However, you might eventually want to expermiment with sequences of characters as tokens, and in this case you'll need a more complex token-to-number mapping, so lets just create a token-to-number mapping scheme now.\n",
        "\n",
        "Since we ordered our unique tokens into the `tokens` list, we can just use the index of the token in the list as the encoding, which is what we do.\n",
        "\n",
        "Note that this does come with some trade-offs though. The position of any given token in the list of tokens is relative to how many and what were the unique tokens in the training data. If we use different trianing data, the tokens might map to different indexes so we need to be careful to preserve our tokens across training data set if we are using mulitple different pieces of training data.\n",
        "\n",
        "Anyway, let's write a token-to-number `encode` function and a number-to-token `decode` function."
      ],
      "metadata": {
        "id": "eENRH2pytS-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, create a mapping of tokens to numbers."
      ],
      "metadata": {
        "id": "x1ybdTibvhg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_to_number = {t: i for i, t in enumerate(tokens)}\n",
        "token_to_number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO-Y4yjkvNM3",
        "outputId": "8b8bd34c-92fb-414f-b80b-82e3d25e8fa8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '$': 3,\n",
              " '&': 4,\n",
              " \"'\": 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '3': 9,\n",
              " ':': 10,\n",
              " ';': 11,\n",
              " '?': 12,\n",
              " 'A': 13,\n",
              " 'B': 14,\n",
              " 'C': 15,\n",
              " 'D': 16,\n",
              " 'E': 17,\n",
              " 'F': 18,\n",
              " 'G': 19,\n",
              " 'H': 20,\n",
              " 'I': 21,\n",
              " 'J': 22,\n",
              " 'K': 23,\n",
              " 'L': 24,\n",
              " 'M': 25,\n",
              " 'N': 26,\n",
              " 'O': 27,\n",
              " 'P': 28,\n",
              " 'Q': 29,\n",
              " 'R': 30,\n",
              " 'S': 31,\n",
              " 'T': 32,\n",
              " 'U': 33,\n",
              " 'V': 34,\n",
              " 'W': 35,\n",
              " 'X': 36,\n",
              " 'Y': 37,\n",
              " 'Z': 38,\n",
              " 'a': 39,\n",
              " 'b': 40,\n",
              " 'c': 41,\n",
              " 'd': 42,\n",
              " 'e': 43,\n",
              " 'f': 44,\n",
              " 'g': 45,\n",
              " 'h': 46,\n",
              " 'i': 47,\n",
              " 'j': 48,\n",
              " 'k': 49,\n",
              " 'l': 50,\n",
              " 'm': 51,\n",
              " 'n': 52,\n",
              " 'o': 53,\n",
              " 'p': 54,\n",
              " 'q': 55,\n",
              " 'r': 56,\n",
              " 's': 57,\n",
              " 't': 58,\n",
              " 'u': 59,\n",
              " 'v': 60,\n",
              " 'w': 61,\n",
              " 'x': 62,\n",
              " 'y': 63,\n",
              " 'z': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then a mapping from numbers to tokens."
      ],
      "metadata": {
        "id": "C3PcZhzXvyzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_to_token = {i: t for i, t in enumerate(tokens)}\n",
        "number_to_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjOQwnVGvtBE",
        "outputId": "3667b8d4-3d4c-4cf7-81ab-2c38e8b48e34"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: ' ',\n",
              " 2: '!',\n",
              " 3: '$',\n",
              " 4: '&',\n",
              " 5: \"'\",\n",
              " 6: ',',\n",
              " 7: '-',\n",
              " 8: '.',\n",
              " 9: '3',\n",
              " 10: ':',\n",
              " 11: ';',\n",
              " 12: '?',\n",
              " 13: 'A',\n",
              " 14: 'B',\n",
              " 15: 'C',\n",
              " 16: 'D',\n",
              " 17: 'E',\n",
              " 18: 'F',\n",
              " 19: 'G',\n",
              " 20: 'H',\n",
              " 21: 'I',\n",
              " 22: 'J',\n",
              " 23: 'K',\n",
              " 24: 'L',\n",
              " 25: 'M',\n",
              " 26: 'N',\n",
              " 27: 'O',\n",
              " 28: 'P',\n",
              " 29: 'Q',\n",
              " 30: 'R',\n",
              " 31: 'S',\n",
              " 32: 'T',\n",
              " 33: 'U',\n",
              " 34: 'V',\n",
              " 35: 'W',\n",
              " 36: 'X',\n",
              " 37: 'Y',\n",
              " 38: 'Z',\n",
              " 39: 'a',\n",
              " 40: 'b',\n",
              " 41: 'c',\n",
              " 42: 'd',\n",
              " 43: 'e',\n",
              " 44: 'f',\n",
              " 45: 'g',\n",
              " 46: 'h',\n",
              " 47: 'i',\n",
              " 48: 'j',\n",
              " 49: 'k',\n",
              " 50: 'l',\n",
              " 51: 'm',\n",
              " 52: 'n',\n",
              " 53: 'o',\n",
              " 54: 'p',\n",
              " 55: 'q',\n",
              " 56: 'r',\n",
              " 57: 's',\n",
              " 58: 't',\n",
              " 59: 'u',\n",
              " 60: 'v',\n",
              " 61: 'w',\n",
              " 62: 'x',\n",
              " 63: 'y',\n",
              " 64: 'z'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can write our encoder."
      ],
      "metadata": {
        "id": "hVd0DxSKv2-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(tokens: str) -> list[int]:\n",
        "  return [token_to_number[t] for t in tokens]\n",
        "\n",
        "encode('apple')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "775B6Tbov2ux",
        "outputId": "baee30be-3737-4e2a-e74f-22b879ad4fce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[39, 54, 54, 50, 43]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the decoder."
      ],
      "metadata": {
        "id": "5RrUVwhlwEB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(numbers: list[int]) -> str:\n",
        "  return ''.join(number_to_token[n] for n in numbers)\n",
        "\n",
        "decode([16, 17, 18])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dfDycwx5wGWF",
        "outputId": "a1c7cb4c-aae1-4ce2-e19c-fe99e3dc1a6c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DEF'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a tokenizer and a large amount of text. Let's tokenize all the text!\n",
        "\n",
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=778)"
      ],
      "metadata": {
        "id": "cTdGKKUD5PZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(raw_training_data), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUZ4022h5Z38",
        "outputId": "6bcfa817-ffcb-4c14-8283-11e471f6f7f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to split training data and testing data."
      ],
      "metadata": {
        "id": "C6w2aQ4-6IYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_size = int(0.9*len(data))\n",
        "training_data = data[:training_data_size]\n",
        "validation_data = data[training_data_size:]\n",
        "print(len(training_data), len(validation_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYVyMdov6MOF",
        "outputId": "7da485ef-d479-4e2d-df0a-65e999cc49a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Dimension"
      ],
      "metadata": {
        "id": "N-ktY-_58eLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we work on training in blocks.\n",
        "In this case we train in blocks of 8 characters, but we always need the \"next\" character, so we work with block sizes of bock+1.\n",
        "\n",
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=913)"
      ],
      "metadata": {
        "id": "8ZdjI7bR7DY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "training_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrxJhNAH7I2u",
        "outputId": "a340f88d-5e06-48b4-d747-dcecf5fbbbba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = training_data[:block_size]\n",
        "y = training_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"in: {context}, out: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB3aEKF77jz1",
        "outputId": "34058735-d3c3-4b1e-f5bb-7925617133e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in: tensor([18]), out: 47\n",
            "in: tensor([18, 47]), out: 56\n",
            "in: tensor([18, 47, 56]), out: 57\n",
            "in: tensor([18, 47, 56, 57]), out: 58\n",
            "in: tensor([18, 47, 56, 57, 58]), out: 1\n",
            "in: tensor([18, 47, 56, 57, 58,  1]), out: 15\n",
            "in: tensor([18, 47, 56, 57, 58,  1, 15]), out: 47\n",
            "in: tensor([18, 47, 56, 57, 58,  1, 15, 47]), out: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Dimension"
      ],
      "metadata": {
        "id": "XScEDiyd8he6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=1129)"
      ],
      "metadata": {
        "id": "SwOcHO4i8sNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = training_data if split == 'train' else validation_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "print('----------------------------------')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f'input {context.tolist()}, target {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDrFtzol8l9R",
        "outputId": "e182f25e-79af-494f-f5db-b2e48103826c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----------------------------------\n",
            "input [24], target 43\n",
            "input [24, 43], target 58\n",
            "input [24, 43, 58], target 5\n",
            "input [24, 43, 58, 5], target 57\n",
            "input [24, 43, 58, 5, 57], target 1\n",
            "input [24, 43, 58, 5, 57, 1], target 46\n",
            "input [24, 43, 58, 5, 57, 1, 46], target 43\n",
            "input [24, 43, 58, 5, 57, 1, 46, 43], target 39\n",
            "input [44], target 53\n",
            "input [44, 53], target 56\n",
            "input [44, 53, 56], target 1\n",
            "input [44, 53, 56, 1], target 58\n",
            "input [44, 53, 56, 1, 58], target 46\n",
            "input [44, 53, 56, 1, 58, 46], target 39\n",
            "input [44, 53, 56, 1, 58, 46, 39], target 58\n",
            "input [44, 53, 56, 1, 58, 46, 39, 58], target 1\n",
            "input [52], target 58\n",
            "input [52, 58], target 1\n",
            "input [52, 58, 1], target 58\n",
            "input [52, 58, 1, 58], target 46\n",
            "input [52, 58, 1, 58, 46], target 39\n",
            "input [52, 58, 1, 58, 46, 39], target 58\n",
            "input [52, 58, 1, 58, 46, 39, 58], target 1\n",
            "input [52, 58, 1, 58, 46, 39, 58, 1], target 46\n",
            "input [25], target 17\n",
            "input [25, 17], target 27\n",
            "input [25, 17, 27], target 10\n",
            "input [25, 17, 27, 10], target 0\n",
            "input [25, 17, 27, 10, 0], target 21\n",
            "input [25, 17, 27, 10, 0, 21], target 1\n",
            "input [25, 17, 27, 10, 0, 21, 1], target 54\n",
            "input [25, 17, 27, 10, 0, 21, 1, 54], target 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram Model"
      ],
      "metadata": {
        "id": "w2bRuY9s-3KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "YqUO5IDzGWae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=1348)\n",
        "\n",
        "[Makemore Series covering the BiGram language model](https://www.youtube.com/watch?v=PaCmpygFfXo)"
      ],
      "metadata": {
        "id": "RN-o8VBS-7PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    B, T, C = logits.shape\n",
        "    logits = logits.view(B*T, C)\n",
        "    targets = targets.view(B*T)\n",
        "    loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "m = BigramLanguageModel(token_count)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssEOig2v-4rF",
        "outputId": "cf45409a-b5b0-432f-bd48-32b63b177a46"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(-np.log(1/65.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMXTrz6PCMoC",
        "outputId": "2dd48919-dec2-4352-ebf0-99f537bd25d0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.174387269895637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=1753)"
      ],
      "metadata": {
        "id": "FixxdvCCC4fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(token_count)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "z = torch.zeros((1,1), dtype=torch.long)\n",
        "print(z)\n",
        "generated = m.generate(z, max_new_tokens=100)\n",
        "print(generated)\n",
        "generated_list = generated[0].tolist()\n",
        "print(generated_list)\n",
        "print(decode(generated_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H59A9IvECueF",
        "outputId": "bfa0044c-ca14-4d38-e555-c1cfca914eca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "tensor([[0]])\n",
            "tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16, 52, 62, 13,  1, 25, 57,  3,  9]])\n",
            "[0, 31, 56, 12, 55, 28, 7, 29, 35, 49, 58, 36, 53, 24, 4, 48, 24, 16, 22, 45, 27, 24, 34, 64, 5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34, 4, 60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63, 39, 53, 8, 55, 44, 64, 57, 3, 37, 57, 3, 64, 18, 7, 61, 6, 11, 43, 17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57, 2, 47, 35, 35, 8, 27, 40, 64, 16, 52, 62, 13, 1, 25, 57, 3, 9]\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ABoqGahpGYi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Reference Point](https://youtu.be/kCc8FmEb1nY?t=2092)"
      ],
      "metadata": {
        "id": "EnltATAZGZwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "qcn8rnsxGcUO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "for steps in range(10000):\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j86g3U1YMfXT",
        "outputId": "dc74c383-59be-4d0e-e4d5-262e6eeeeaae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.559082508087158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.zeros((1,1), dtype=torch.long)\n",
        "print(z)\n",
        "generated = m.generate(z, max_new_tokens=100)\n",
        "print(generated)\n",
        "generated_list = generated[0].tolist()\n",
        "print(generated_list)\n",
        "print(decode(generated_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xEql-p7NGLf",
        "outputId": "b39b36b8-249d-4ef9-ee65-5e052856ad88"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0]])\n",
            "tensor([[ 0, 13, 52, 41, 53,  1, 57,  1, 39, 54, 56, 45, 56,  1, 39, 50, 57,  1,\n",
            "         53, 56, 43, 56, 60, 43,  6,  1, 63, 53,  1, 47, 42, 47, 50, 39, 41, 39,\n",
            "         47, 39,  1, 44,  1, 58, 50, 50, 47, 52, 45, 47, 58, 56, 43, 63,  1, 50,\n",
            "          1, 53, 59, 43, 60, 43, 56, 58,  1, 15, 33, 30, 53, 56, 42,  1, 40, 43,\n",
            "         39,  1, 39, 56, 53, 59, 50, 50, 39, 47, 53, 61, 53, 52, 45, 43,  6,  1,\n",
            "         57,  8,  0, 25, 63,  1, 63,  1, 48, 53, 51]])\n",
            "[0, 13, 52, 41, 53, 1, 57, 1, 39, 54, 56, 45, 56, 1, 39, 50, 57, 1, 53, 56, 43, 56, 60, 43, 6, 1, 63, 53, 1, 47, 42, 47, 50, 39, 41, 39, 47, 39, 1, 44, 1, 58, 50, 50, 47, 52, 45, 47, 58, 56, 43, 63, 1, 50, 1, 53, 59, 43, 60, 43, 56, 58, 1, 15, 33, 30, 53, 56, 42, 1, 40, 43, 39, 1, 39, 56, 53, 59, 50, 50, 39, 47, 53, 61, 53, 52, 45, 43, 6, 1, 57, 8, 0, 25, 63, 1, 63, 1, 48, 53, 51]\n",
            "\n",
            "Anco s aprgr als orerve, yo idilacaia f tllingitrey l ouevert CURord bea aroullaiowonge, s.\n",
            "My y jom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When the code is converted to a script\n"
      ],
      "metadata": {
        "id": "yxuEmNgQNnpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Checkpoint](https://youtu.be/kCc8FmEb1nY?t=2257)"
      ],
      "metadata": {
        "id": "uOWmir7ENRJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKNXTIm2NrRT",
        "outputId": "2f1c9c8e-739a-4d06-86e7-64f71de307bb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 losses {'train': tensor(4.7305), 'val': tensor(4.7241)}\n",
            "iter 300 losses {'train': tensor(2.8101), 'val': tensor(2.8243)}\n",
            "iter 600 losses {'train': tensor(2.5400), 'val': tensor(2.5652)}\n",
            "iter 900 losses {'train': tensor(2.4896), 'val': tensor(2.5060)}\n",
            "iter 1200 losses {'train': tensor(2.4830), 'val': tensor(2.5012)}\n",
            "iter 1500 losses {'train': tensor(2.4631), 'val': tensor(2.4905)}\n",
            "iter 1800 losses {'train': tensor(2.4655), 'val': tensor(2.4926)}\n",
            "iter 2100 losses {'train': tensor(2.4670), 'val': tensor(2.4840)}\n",
            "iter 2400 losses {'train': tensor(2.4612), 'val': tensor(2.4879)}\n",
            "iter 2700 losses {'train': tensor(2.4717), 'val': tensor(2.4915)}\n",
            "\n",
            "od nos CAy go ghanoray t, co haringoudrou clethe k, leve fr werar,\n",
            "Is fa!\n",
            "\n",
            "\n",
            "Thilemel cia h hmboomyorarifrcitheviPO, tle dst f qur'dig t cof boddo y t o ar pileas h mo wierl t,\n",
            "S:\n",
            "STENENEat I athe thounomy tinrent distesisanimald 3I'leliento ald, avavis nofrisist me Busarend un'soto iat s k,\n",
            "SBRI he the f wendleindd t acoe ts ansur thy ppr h.\n",
            "\n",
            "\n",
            "Y:\n",
            "KIIsqu pr odEd ch,\n",
            "APrnes ouse bll owhored miner t ooon'stoume bupromo! fifoveghind hiarnge s.\n",
            "MI aswimy or m, wardd tw'To tee abifewoetsphin sed The a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Math Trick Self-Attention"
      ],
      "metadata": {
        "id": "smnTceY1UmTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgoEoP4TUouq",
        "outputId": "8f64b672-c06f-4445-b797-f53630aac202"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1]\n",
        "    xbow[b,t] = torch.mean(xprev, 0)\n",
        "\n",
        "x[0], xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_JX5N6aVbCL",
        "outputId": "24743b06-22e4-4acb-8334-8b5f0f4d5b4b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700],\n",
              "         [-0.3596, -0.9152],\n",
              "         [ 0.6258,  0.0255],\n",
              "         [ 0.9545,  0.0643],\n",
              "         [ 0.3612,  1.1679],\n",
              "         [-1.3499, -0.5102],\n",
              "         [ 0.2360, -0.2398],\n",
              "         [-0.9211,  1.5433]]),\n",
              " tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.ones(3,3)\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('----------------')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('----------------')\n",
        "print('c=')\n",
        "print(c)\n",
        "print('----------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwjMV2GdV6el",
        "outputId": "7d27fd4f-bc4f-4789-cc83-cc79453d9d84"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "----------------\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "----------------\n",
            "c=\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tril(torch.ones(3,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4qePecXWSlE",
        "outputId": "c4bf3ae4-2618-4eca-ae3a-ab8f5cb15d73"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('----------------')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('----------------')\n",
        "print('c=')\n",
        "print(c)\n",
        "print('----------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n-qd_7qWinG",
        "outputId": "e6d64f9f-658f-42c0-bc55-8af9c5068b85"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "----------------\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "----------------\n",
            "c=\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('----------------')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('----------------')\n",
        "print('c=')\n",
        "print(c)\n",
        "print('----------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdsWB8LTW97V",
        "outputId": "f777a413-cab2-4432-bbad-8a9c49097a24"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "----------------\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "----------------\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm0fIChRXQ2F",
        "outputId": "e43f57a7-3771-4230-e9b6-04c24d38924a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C)\n",
        "torch.allclose(xbow[0], xbow2[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sizFtzMeXdoe",
        "outputId": "caa67c79-23f0-4302-8447-f36ac342be59"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0], xbow2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpNq46uoX4Zp",
        "outputId": "d3c52c82-77a3-4f41-d01b-cb1928728be3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]),\n",
              " tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow[0], xbow3[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATxsDtI8YORR",
        "outputId": "82ae0f18-16a5-43dc-c841-8c0e97433eea"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Self-Attention"
      ],
      "metadata": {
        "id": "XINMkAIyZm7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Checkpoint](https://youtu.be/kCc8FmEb1nY?t=3510)"
      ],
      "metadata": {
        "id": "TrDkjivdZtPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb\n",
        "    logits = self.lm_head(tok_emb)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "#print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQhO50z5Zo15",
        "outputId": "2ae8017f-cd63-479e-a17b-b657c076ec4d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0; train loss 4.3446; val loss 4.3516\n",
            "iter 300; train loss 2.5224; val loss 2.5419\n",
            "iter 600; train loss 2.5000; val loss 2.5259\n",
            "iter 900; train loss 2.4878; val loss 2.5083\n",
            "iter 1200; train loss 2.5051; val loss 2.5232\n",
            "iter 1500; train loss 2.4801; val loss 2.5083\n",
            "iter 1800; train loss 2.4902; val loss 2.5192\n",
            "iter 2100; train loss 2.4917; val loss 2.5077\n",
            "iter 2400; train loss 2.4890; val loss 2.5103\n",
            "iter 2700; train loss 2.4999; val loss 2.5110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More on self-attention"
      ],
      "metadata": {
        "id": "xN9_KjTTcCtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Checkpoint](https://youtu.be/kCc8FmEb1nY?t=3728)"
      ],
      "metadata": {
        "id": "axw4HsAhcFQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "B,T,C = 4,8,32\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros(T,T)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDEbrl2fcJ-9",
        "outputId": "19a6a249-523f-40c7-84e2-f1805f1d1e1a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzw_2TywdAJX",
        "outputId": "26932ce8-1e72-47e4-bfaa-847c6a261e91"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psvHBNkmdBCk",
        "outputId": "9ea8d50c-56e5-4cc5-cf98-6e96ddff96e2"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emit two vectors: query and key; what am I looking for and what do I contain?"
      ],
      "metadata": {
        "id": "APUS9LpLdQ_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#wei = torch.zeros(T,T)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eXbUtG_de7k",
        "outputId": "b02c3bd0-2f5d-4e4d-bf9f-3ab4824d2775"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XreI7GEeJ3N",
        "outputId": "2a6bbfd2-f084-4217-9ae4-5a905ec4a861"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
              "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
              "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
              "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
              "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
              "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
              "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# wei = torch.zeros(T,T)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fuk4M56fENJ",
        "outputId": "78216261-02f6-46a6-c9a5-e789843918e8"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYRm-3UpfHQ6",
        "outputId": "f697a9c8-a98a-42d6-cb33-430fa33100a9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
              "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
              "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
              "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
              "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
              "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
              "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating Self-Attention"
      ],
      "metadata": {
        "id": "JWhgmbXtfZRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Checkpoint](https://youtu.be/kCc8FmEb1nY?t=4751)"
      ],
      "metadata": {
        "id": "Vlds7fE4hJyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.sa_head = Head(n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_head(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fq9II6dhT2k",
        "outputId": "35401bea-744e-43b5-b3c8-fdbb78ea5458"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0; train loss 4.2000; val loss 4.2047\n",
            "iter 300; train loss 2.9407; val loss 2.9563\n",
            "iter 600; train loss 2.6300; val loss 2.6448\n",
            "iter 900; train loss 2.5378; val loss 2.5445\n",
            "iter 1200; train loss 2.5034; val loss 2.5029\n",
            "iter 1500; train loss 2.4690; val loss 2.4805\n",
            "iter 1800; train loss 2.4516; val loss 2.4641\n",
            "iter 2100; train loss 2.4380; val loss 2.4421\n",
            "iter 2400; train loss 2.4236; val loss 2.4416\n",
            "iter 2700; train loss 2.4115; val loss 2.4338\n",
            "iter 3000; train loss 2.4222; val loss 2.4239\n",
            "iter 3300; train loss 2.4152; val loss 2.4229\n",
            "iter 3600; train loss 2.4050; val loss 2.4175\n",
            "iter 3900; train loss 2.4016; val loss 2.4176\n",
            "iter 4200; train loss 2.3949; val loss 2.4302\n",
            "iter 4500; train loss 2.3879; val loss 2.4021\n",
            "iter 4800; train loss 2.3830; val loss 2.4041\n",
            "\n",
            "-bem, ph, ang rmp dlules,\n",
            "STESORCD:\n",
            "FULPUS:\n",
            "Kofr fir! my,\n",
            "Inot thand thur aw ewies; ne st t theaked?\n",
            "CARLO:\n",
            "WAUCES:\n",
            "TAnguli nd, whepathil walldernerf ndes stur his atst bus, har m'Tits, stt eplcpereer se the the mtexas to\n",
            "Me? od tot froe Aurr is pttemest a lwel houl ll; thatsch, haticche thanker in:\n",
            "Wa ger we us min; to tak eat oud bthba bor win.\n",
            "Tore Wand;\n",
            "I:\n",
            "BRHY:\n",
            "Wind I noldscy,\n",
            "D:\n",
            "Versing sothat verele bim by cer R'Thing shaint hat ther!\n",
            "thir, tifa vegormeand sfard gralden st minderr'd ldlll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention"
      ],
      "metadata": {
        "id": "1T4Smhh9lSs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Checkpoint](https://youtu.be/kCc8FmEb1nY?t=4920)"
      ],
      "metadata": {
        "id": "_f47LofHlVBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.sa_heads = MultiHeadAttention(4, n_embed//4)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_heads(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZmxMKqclY8C",
        "outputId": "85251a94-1778-4310-e4cd-f206d4ecbd14"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0; train loss 4.2227; val loss 4.2226\n",
            "iter 300; train loss 2.8394; val loss 2.8474\n",
            "iter 600; train loss 2.6065; val loss 2.6170\n",
            "iter 900; train loss 2.5201; val loss 2.5228\n",
            "iter 1200; train loss 2.4688; val loss 2.4680\n",
            "iter 1500; train loss 2.4187; val loss 2.4283\n",
            "iter 1800; train loss 2.3934; val loss 2.4005\n",
            "iter 2100; train loss 2.3639; val loss 2.3668\n",
            "iter 2400; train loss 2.3408; val loss 2.3582\n",
            "iter 2700; train loss 2.3227; val loss 2.3441\n",
            "iter 3000; train loss 2.3213; val loss 2.3236\n",
            "iter 3300; train loss 2.3060; val loss 2.3141\n",
            "iter 3600; train loss 2.2877; val loss 2.3077\n",
            "iter 3900; train loss 2.2813; val loss 2.3005\n",
            "iter 4200; train loss 2.2731; val loss 2.3052\n",
            "iter 4500; train loss 2.2593; val loss 2.2738\n",
            "iter 4800; train loss 2.2498; val loss 2.2721\n",
            "\n",
            "-be trow, ang rop alrame,\n",
            "Sorsolts rea whoutoo-\n",
            "GBin! my ranmy thlay ther an elies; ne shat theakedce singhaill dre sm lulis my whip I me wall-Vingth not bre revir atst bus frow me dis hent ef cperre. Be wit Gle moustien,\n",
            "Me? picr thas cou thill proe kst a liell folt my theasch, hatich fely hater, sot wald we uss inged pins eat oud bt ba bos wit.\n",
            "Tore WEnd ad:\n",
            "Be, my cat I noldscy,\n",
            "D:\n",
            "Vens\n",
            "Thee thet vis learim byporr Rachvells boot hit ther!\n",
            "thir, torat mis meand sfarl graldelle so therr'd lell \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Feed-Forward"
      ],
      "metadata": {
        "id": "f51-szLKmuV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Bookmark](https://youtu.be/kCc8FmEb1nY?t=5113)"
      ],
      "metadata": {
        "id": "ugxIGRCwmx4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, n_embed),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.sa_heads = MultiHeadAttention(4, n_embed//4)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_heads(x)\n",
        "    x = self.ffwd(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-za50IgmwNG",
        "outputId": "758d2899-3426-47cb-b5ac-04518f1dc3e3"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0; train loss 4.1996; val loss 4.1995\n",
            "iter 300; train loss 2.7604; val loss 2.7730\n",
            "iter 600; train loss 2.5627; val loss 2.5723\n",
            "iter 900; train loss 2.4921; val loss 2.4782\n",
            "iter 1200; train loss 2.4399; val loss 2.4340\n",
            "iter 1500; train loss 2.3895; val loss 2.3987\n",
            "iter 1800; train loss 2.3601; val loss 2.3653\n",
            "iter 2100; train loss 2.3321; val loss 2.3408\n",
            "iter 2400; train loss 2.3085; val loss 2.3311\n",
            "iter 2700; train loss 2.2819; val loss 2.3028\n",
            "iter 3000; train loss 2.2832; val loss 2.2868\n",
            "iter 3300; train loss 2.2728; val loss 2.2758\n",
            "iter 3600; train loss 2.2454; val loss 2.2716\n",
            "iter 3900; train loss 2.2349; val loss 2.2653\n",
            "iter 4200; train loss 2.2242; val loss 2.2674\n",
            "iter 4500; train loss 2.2147; val loss 2.2406\n",
            "iter 4800; train loss 2.2083; val loss 2.2273\n",
            "\n",
            "Fondsle, whice,\n",
            "Clltre luisY lion:\n",
            "Bos daunot seind a gong woulld ris. Pund.\n",
            "\n",
            "LOGLARBIRD GRDENILOME:\n",
            "Sin the right thodes\n",
            "We\n",
            "Thead, s aw sicht wothilbove it baytin,\n",
            "In.\n",
            "\n",
            "Bith ever'ed rouse,\n",
            "At doldilingut, fak co.\n",
            "Ohe dutsh.\n",
            "\n",
            "WAShy hade, fund mit me chewed' had.\n",
            "\n",
            "DUTo beffer'ss, shond o' tore of ther of Ely hat is Mry atollll nothine do will kungileing.\n",
            "Mill twith gard sou;\n",
            "Nousw'ld coucht wer'kecy wis goongut, gend Hasts, my theer rop restare is che wamienome meamabe.\n",
            "ALYIRK:\n",
            "Notth\n",
            "Then noy mon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TBD"
      ],
      "metadata": {
        "id": "zVIJp1VEnxRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Bookmark](https://youtu.be/kCc8FmEb1nY?t=5212)"
      ],
      "metadata": {
        "id": "ICpGGHESnzYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Mult\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, n_embed),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sa(x)\n",
        "    x = self.ffwd(x)\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4),\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLAR5bSJn14x",
        "outputId": "35d458a4-4d12-4b1c-bcc3-7e55bc06708c"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0; train loss 4.2116; val loss 4.2078\n",
            "iter 300; train loss 3.2365; val loss 3.2666\n",
            "iter 600; train loss 2.9677; val loss 2.9683\n",
            "iter 900; train loss 2.7366; val loss 2.7402\n",
            "iter 1200; train loss 2.6278; val loss 2.6364\n",
            "iter 1500; train loss 2.5361; val loss 2.5374\n",
            "iter 1800; train loss 2.5012; val loss 2.4985\n",
            "iter 2100; train loss 2.4601; val loss 2.4599\n",
            "iter 2400; train loss 2.4394; val loss 2.4459\n",
            "iter 2700; train loss 2.4248; val loss 2.4324\n",
            "iter 3000; train loss 2.4069; val loss 2.4102\n",
            "iter 3300; train loss 2.3922; val loss 2.3848\n",
            "iter 3600; train loss 2.3721; val loss 2.3683\n",
            "iter 3900; train loss 2.3591; val loss 2.3694\n",
            "iter 4200; train loss 2.3573; val loss 2.3722\n",
            "iter 4500; train loss 2.3459; val loss 2.3393\n",
            "iter 4800; train loss 2.3118; val loss 2.3355\n",
            "\n",
            "Wot elve sot geir some, ovenry pug meler worlive a now ame! in yord ase theen suest to prreat try I crour nemese a qnel thom, poun bud hellear dove be myy don cand havl creamser tinse,\n",
            "Piless ther, mare:\n",
            "Ond\n",
            "Woledlfilce thy eveere dratath pry he bruteg boiese.\n",
            "\n",
            "ARILI'th thy ing;\n",
            "Shat thy you.\n",
            "\n",
            "To\n",
            "MICermtath grel dead.\n",
            "\n",
            "ASABRMTES:\n",
            "Yo binle boloumens.\n",
            "\n",
            "Fheres.\n",
            "\n",
            "OTent endetees rint liosut meesson martle agsh, sresttes'od,\n",
            "I me!\n",
            "Wo pambneay and a to moouy.\n",
            "\n",
            "QUWRIUEI:\n",
            "HIaw craotatoth hard iot bote fe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fix Optimization"
      ],
      "metadata": {
        "id": "eBxwuKQ0pECs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Bookmark](https://youtu.be/kCc8FmEb1nY?t=5314)"
      ],
      "metadata": {
        "id": "u4PCqBu9pGdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Mult\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed, n_embed),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(x)\n",
        "    x = x + self.ffwd(x)\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4),\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwRDOo2EpJmm",
        "outputId": "06988341-e3f7-447b-d98a-7a95a43e0bd1"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0; train loss 4.6255; val loss 4.6233\n",
            "iter 300; train loss 2.4841; val loss 2.4842\n",
            "iter 600; train loss 2.3605; val loss 2.3646\n",
            "iter 900; train loss 2.2841; val loss 2.2939\n",
            "iter 1200; train loss 2.2150; val loss 2.2428\n",
            "iter 1500; train loss 2.1835; val loss 2.2162\n",
            "iter 1800; train loss 2.1524; val loss 2.1898\n",
            "iter 2100; train loss 2.1145; val loss 2.1594\n",
            "iter 2400; train loss 2.1046; val loss 2.1540\n",
            "iter 2700; train loss 2.1009; val loss 2.1502\n",
            "iter 3000; train loss 2.0752; val loss 2.1306\n",
            "iter 3300; train loss 2.0570; val loss 2.1247\n",
            "iter 3600; train loss 2.0395; val loss 2.1040\n",
            "iter 3900; train loss 2.0418; val loss 2.1170\n",
            "iter 4200; train loss 2.0319; val loss 2.0880\n",
            "iter 4500; train loss 2.0206; val loss 2.0954\n",
            "iter 4800; train loss 1.9812; val loss 2.0793\n",
            "\n",
            "But, agains setted beld the benot poyther.\n",
            "\n",
            "Fon; We yerboun efeen'' takemble? old taking this faither they not me, beed,\n",
            "Seace\n",
            "Hervile!\n",
            "Our, porrefly on; eet a tuth shre if for I hild bitHinged diturst dead sir,\n",
            "When'd Dy GLARD:\n",
            "I:\n",
            "Ay, that bettry would.\n",
            "\n",
            "LEONIO:\n",
            "Sow:\n",
            "Thank spey,\n",
            "And ker brodver, intight, warl thou go;\n",
            "The boot, lovick, WIry, me for joby sweas becutes\n",
            "Didve i milly brefrime thee.\n",
            "\n",
            "DUKE RIVENCENTH:\n",
            "Bethen, rive\n",
            "The Silless her brothers theel'd slep Murden thy for jube fordang bru\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LayerNorm"
      ],
      "metadata": {
        "id": "J2rDPVETq4Z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video Bookmark](https://youtu.be/kCc8FmEb1nY?t=5598)"
      ],
      "metadata": {
        "id": "cTD8FtUhq84c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize to zero mean and 1 standard deviation."
      ],
      "metadata": {
        "id": "2s2C8Fs5rHSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Mult\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed, n_embed),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4),\n",
        "        nn.LayerNorm(n_embed),\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PmGtf0BrE-D",
        "outputId": "f10410ef-795f-4d6d-cd0e-2794191a6658"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0; train loss 4.3103; val loss 4.3097\n",
            "iter 300; train loss 2.5213; val loss 2.5307\n",
            "iter 600; train loss 2.3629; val loss 2.3655\n",
            "iter 900; train loss 2.2726; val loss 2.2830\n",
            "iter 1200; train loss 2.1975; val loss 2.2302\n",
            "iter 1500; train loss 2.1675; val loss 2.2025\n",
            "iter 1800; train loss 2.1355; val loss 2.1764\n",
            "iter 2100; train loss 2.1020; val loss 2.1443\n",
            "iter 2400; train loss 2.0918; val loss 2.1350\n",
            "iter 2700; train loss 2.0800; val loss 2.1305\n",
            "iter 3000; train loss 2.0578; val loss 2.1073\n",
            "iter 3300; train loss 2.0381; val loss 2.1035\n",
            "iter 3600; train loss 2.0301; val loss 2.0973\n",
            "iter 3900; train loss 2.0298; val loss 2.0905\n",
            "iter 4200; train loss 2.0115; val loss 2.0708\n",
            "iter 4500; train loss 2.0061; val loss 2.0820\n",
            "iter 4800; train loss 1.9745; val loss 2.0692\n",
            "\n",
            "But, agn ceas'th thee quent beninguont he thign; ke yedbrus leevis' breimed to kestand,\n",
            "Wit suchie,\n",
            "With the was:\n",
            "A my good,\n",
            "Make on archt!\n",
            "\n",
            "pore fly on; rethat:\n",
            "Whis buk pom.\n",
            "\n",
            "ROMENIE:\n",
            "I alied di'gis conge sir,\n",
            "Pronces: he somes dess, that bettry would.\n",
            "\n",
            "LETER:\n",
            "Oo veer.\n",
            "\n",
            "LUCINIUS: Pripost sever, intiontens.\n",
            "They scex;\n",
            "The bod not nik. Belly, misteld siges lase I:\n",
            "Mether\n",
            "Po i mile this with speor a trustresser!\n",
            "\n",
            "KING\n",
            "Nugh'd a ques Side,\n",
            "Let:\n",
            "Abrother: the looks: pleake,\n",
            "Away for mist fom ang bre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scale Up"
      ],
      "metadata": {
        "id": "bL2gJpHwsWmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Mult\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed, n_embed),\n",
        "        nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmNikMqvsYCW",
        "outputId": "60a687e6-cb1f-4bc9-cb2d-0d9abe66f57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0; train loss 4.2849; val loss 4.2823\n",
            "iter 500; train loss 2.0106; val loss 2.0964\n"
          ]
        }
      ]
    }
  ]
}